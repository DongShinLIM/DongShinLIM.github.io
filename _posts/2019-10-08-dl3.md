---
title: 'Deep Learning'
date: 2019-10-08 00:00:00
featured_image: '/images/demo/dl.jpg'
excerpt: 1-3 Data representations for neural networks
---

### 2.3. 신경망의 톱니바퀴 : 텐서 연산

- 심층 신경망이 학습한 모든 변환을 수치 데이터 텐서에 적용하는 몇 종류의 텐서 연산 **tensor operation**으로 나타낼 수 있습니다.
- 예를 들어, 텐서 덧셈이나 텐서 곱셈입니다.

앞 게시물에서 Dense 층을 쌓아서 신경망을 만들었습니다. 케라스의 층은 다음과 같이 생성합니다.

keras.layers.Dense(512, activation='relu')

- 이 층은 2D 텐서를 입력 받고 입력 텐서의 새로운 표현인 또 다른 2D 텐서를 반환하는 함수로 해석할 수 있습니다. 

- 구체적으로, 이 함수는 다음과 같습니다. (W는 2D 텐서이고  b는 벡터 입니다. 둘 모두 층의 속성입니다.)

output = relu(dot(W, input) + b)

좀 더 자세히 알아보면 다음과 같습니다.

**여기에 3개의 텐서 연산이 있습니다.:** 
- 입력 텐서와 텐서 W사이의 점곱(dot); 
- 점곱의 결과인 2D 텐서와 벡터 b 사이의 덧셈(+); 
- 마지막으로 relu 연산입니다. relu(x)는 max(x,0)입니다.

### 2.3.1. 원소별 원산(모든 entry에 적용)

- relu 함수와 덧셈은 원소별 연산입니다: 
- 이 연산은 텐서에 있는 각 원소에 독립적으로 적용됩니다. 
- 이 말은 고도의 병렬구현이 가능한 연산이라는 의미입니다.

- 만약 파이썬으로 원소별 연산을 구현한다면 다음 relu 연산 구현처럼 for 반복문을 사용 할 것입니다:


```python
def naive_relu(x):
    assert len(x.shape) == 2                     #  x는 2D 넘파이 배열입니다.

    x = x.copy()                                 #  입력 텐서 자체를 바꾸지 않도록 복사합니다.
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] = max(x[i, j], 0)
    return x
```

덧셈도 동일합니다.


```python
def naive_add(x, y):
    assert len(x.shape) == 2
    assert x.shape == y.shape                # x는 2D 넘파이 배열입니다.

    x = x.copy()
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[i, j]
    return x
```

같은 원리로 원소별 곱셈, 뺄셈 등을 할 수 있습니다.

- 사실 넘파이 배열을 다룰 때는 최적화된 넘파이 내장 함수로 이런 연산들을 처리할 수 있습니다.

- 따라서 넘파이는 다음과 같은 원소별 연산을 엄청난 속도로 처리합니다:


```python
import numpy as np

z = x + y                       # Element-wise Addition

z = np.maximum(z, 0.)           # Element-wise relu
```

### 2.3.2. 브로드캐스팅

- 앞서 살펴본 단순한 덧셈 구현인 naive_add는 동일한 크기의 2D 텐서만 지원합니다. 
- 하지만 이전에 보았던 Dense 층에서는 2D 텐서와 벡터를 더했습니다. 

#### 크기가 다른 두 텐서를 더하면 어떤 일이 일어날까요?

실행 가능하다면, 작은 텐서가 큰 텐서의 크기에 맞추어 브로드캐스팅 됩니다. 이는 두 단계로 이루어집니다.

1. 큰 텐서의 ndim에 맞도록 작은 텐서에 (브로드캐스팅 축이라고 부르는) 축이 추가 됩니다.
2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됩니다. 

#### 구체적인 예를 살펴보겠습니다.

- X의 크기는 (32,10)이고 y의 크기는 (10, )이라고 가정합니다.
- 먼저 y에 비어 있는 첫 번째 축을 추가하여 크기를 (1,10)으로 만듭니다. 
- 그 다음 y를 이 축에 32번 반복하면 텐서 Y의 크기는 (32,10)이 됩니다. 여기에서 Y[i, :] == y for i in range(0,32)입니다.
- 이제 X와 Y의 크기가 같으므로 더할 수 있습니다.

- 구현 입장에서 보면 새로운 텐서가 만들어지면 매우 비효율 적이므로 어떤 2D 텐서도 만들어지지 않습니다.
- 반복된 연산은 매우 가상적이며, 이 과정은 메모리 수준이 아니라 알고리즘 수준에서 일어납니다.
- 하지만 새로운 축을 따라 벡터가 32번 반복된다고 생각하는 것이 이해하기 쉽습니다.

### 다음은 단순하게 구현한 예 입니다.

```python
def naive_add_matrix_and_vector(x, y):
    assert len(x.shape) == 2                           #  x는 넘파이 배열입니다.
    assert len(y.shape) == 1                           #  y는 넘파이 배열입니다.
    assert x.shape[1] == y.shape[0]

    x = x.copy()                                       #  입력 텐서를 바꾸지 않도록 복사합니다.
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[j]
    return x
```

- (a, b, ... n, n + 1, ... m)크기의 텐서와 (n, n + 1, ... m)크기의 텐서 사이에 브로드캐스팅으로 원소별 연산을 적용할 수 있습니다. 

- 이 때 브로드캐스팅은 a부터 n-1까지의 축에 자동으로 일어납니다.

#### 다음은 크기가 다른 두 텐서에 브로드캐스팅으로 원소별 maximum 연산을 적용하는 예입니다.


```python
import numpy as np

x = np.random.random((64, 3, 32, 10))        # x는 (64, 3, 32, 10)크기의 랜덤 텐서입니다.
y = np.random.random((32, 10))              #  y는 (32, 10)크기의 랜덤 텐서입니다.

z = np.maximum(x, y)                        # 출력 z크기는 x와 동일하게 (64, 3, 32, 10) 입니다.
```

### 2.3.3. 텐서 점곱

- 텐서 곱셈이라고 부르는 (원소별 곱셈과 혼동하지 마세요) 점곱 연산은 가장 널리 사용되고 유용한 텐서 연산입니다.
- 원소별 연산과 반대로 입력 텐서의 원소들을 결합시킵니다.

- 넘파이, 케라스, 씨아노, 텐서플로에서 원소별 곱셈은 * 연산자를 사용합니다.
- 텐서플로에서는 dot 연산자가 다르지만 넘파이와 케라스는 점곱 연산에 보편적인 dot 연산자를 사용합니다. 

import numpy as np
z = np.dot(x, y)

from keras import backend as K
z = K.dot(x, y)

z = x . y

점곱 연산은 수학에서 어떤 일을 할까요? 2개의 벡터 x와 y의 점곱은 다음과 같이 계산을 합니다.


```python
def naive_vector_dot(x, y):
    assert len(x.shape) == 1
    assert len(y.shape) == 1
    assert x.shape[0] == y.shape[0]

    z = 0.
    for i in range(x.shape[0]):
        z += x[i] * y[i]
    return z
     
    # x와 y는 넘파이 벡터입니다.
```

- 여기서 볼 수 있듯이 두 벡터의 점곱은 스칼라가 되므로 원소 개수가 같은 벡터끼리 점곱이 가능합니다.

행렬 x와 벡터 y사이에서도 점곱이 가능합니다. y와 x의 행 사이에서 점곱이 일어나므로 벡터가 반환됩니다. 


다음과 같이 구현할 수 있습니다:


```python
import numpy as np

def naive_matrix_vector_dot(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 1
    assert x.shape[1] == y.shape[0]

    z = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            z[i] += x[i, j] * y[j]
    return z
```

행렬-벡터 점곱과 벡터-벡터 점곱 사이의 관계를 부각하기 위해 앞에서 만든 함수를 재사용해 보겠습니다.


```python
def naive_matrix_vector_dot(x, y):
    z = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        z[i] = naive_vector_dot(x[i, :], y)
    return z
```

두 텐서 중 하나라도 ndim이 1보다 크면 dot 연산에 교환 법칙이 성립되지 않습니다. 

다시말하면 dot(x,y)와 dot(y,x)가 같지 않습니다.

- 물론 점곱은 임의의 축 개수를 가진 텐서에 일반화됩니다. 가장 일반적인 용도는 두 행렬 간의 점곱일 것입니다.
- x.shape[1] == y.shape[0] 일 때 두 행렬 x와 y의 점곱(dot(x,y))이 성립됩니다. 
- x의 열과 y의 행 사이 벡터의 점곱으로 인해 (x.shape[0], y.shape[1]) 크기의 행렬이 됩니다. 
다음은 단순한 구현 예 입니다.


```python
def naive_matrix_dot(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 2
    assert x.shape[1] == y.shape[0]

    z = np.zeros((x.shape[0], y.shape[1]))  # 이 연산은 0이 채워진 특정 크기의 벡터를 만듭니다.
    for i in range(x.shape[0]):             # x의 행을 반복합니다.
        for j in range(y.shape[1]):         # y의 열을 반복합니다.
            row_x = x[i, :]
            column_y = y[:, j]
            z[i, j] = naive_vector_dot(row_x, column_y)
    return z
```

다음 그림과 같이 입력과 출력을 배치해 보면 어떤 크기의 점곱이 가능한지 이해하는데 도움이 됩니다.

![](/images/demo/Fig2-5.PNG)

- x, y, z는 직사각형 모양으로 그려져 있습니다.(원소들이 채워진 박스라고 생각하면 됩니다)
- x의 행 벡터와 y의 열 벡터가 같은 크기여야 하므로 자동으로 x의 너비는 y의 높이와 동일해야 합니다.

더 일반적으로 앞서 설명한 2D의 경우처럼 크기를 맞추는 동일한 규칙을 따르면 다음과 같이 고차원 텐서 간의 점곱을 할 수 있습니다.

(a, b, c, d) . (d,) -> (a, b, c)

(a, b, c, d) . (d, e) -> (a, b, c, e)

### 2.3.4. 텐서 크기 변환

- 반드시 알아 두어야 할 세 번째 텐서 연산은 **텐서 크기 변환** 입니다.
- 첫 번째 신경망 예제의 Dense 층에서는 사용되지 않지만 신경망에 주입할 숫자 데이터를 전처리 할 때 사용했습니다. 


```python
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
```

```python
train_images.shape
```

    (60000, 28, 28)

```python
train_images = train_images.reshape((60000, 28 * 28))
```

```python
train_images.shape
```

    (60000, 784)



- 텐서의 크기를 변환 한다는 것은 특정 크기에 맞게 열과 행을 재배열 한다는 것입니다.
- 당연히 크기가 변환된 텐서는 원래 텐서와 원소 개수가 동일 합니다. 

간단한 예제를 통해 크기 변환을 알아보겠습니다:


```python
x = np.array([[0., 1.],
              [2., 3.],
              [4., 5.]])
print(x.shape)
```

    (3, 2)

```python
x = x.reshape((6, 1))
x
```

    array([[0.],
           [1.],
           [2.],
           [3.],
           [4.],
           [5.]])

```python
x.shape
```

    (6, 1)


```python
x = x.reshape((2, 3))
x
```

    array([[0., 1., 2.],
           [3., 4., 5.]])


```python
x.shape
```

    (2, 3)



- 자주 사용하는 특별한 크기 변환은 **전치**입니다.
- 행렬의 전치는 행과 열을 바꾸는 것을 의미합니다.

```python
x = np.zeros((300, 20))

x = np.transpose(x)

print(x.shape)
```

    (20, 300)


### 2.3.5. 텐서 연산의 기하학적 해석

- 텐서 연산이 조작하는 텐서의 내용은 어떤 기하학적 공간에 있는 좌표 포인트로 해석될 수 있기 때문에 모든 텐서 연산은 기하학적 해석이 가능합니다.
- 예를 들어 덧셈을 생각해보면서 다음 벡터를 보겠습니다.

```python
A = [0.5, 1]
```

이 포인트는 2D 공간에 있습니다(see figure 2.6). 일반적으로 그림 2-7과 같이 원점에서 포인트를 연결하는 화살표로 벡터를 나타냅니다.

![](/images/demo/Fig2-6.PNG)
![](/images/demo/Fig2-7.PNG)

- 새로운 포인트, B = [1, 0.25]를 이전 벡터에 더해 보겠습니다.
- 기하학적으로는 벡터 화살표를 연결하여 계산할 수 있습니다. 최종 위치는 두 벡터의 덧셈을 나타내는 벡터가 됩니다. (그림 2-8 참조)

![](/images/demo/Fig2-8.PNG)


### 2.3.6. 딥러닝의 기하학적 해석

- **신경망은 전체적으로 텐서 연산의 연결로 구성된 것이고, 모든 텐서 연산은 입력 데이터의 기하학적 변환** 임을 배웠습니다.
- 단순한 단계들이 길게 이어져 구현된 신경망을 고차원 공간에서 매우 복잡한 기하학적 변환을 하는 것으로 해석할 수 있습니다.

3D라면 다음 비유가 이해하는데 도움이 될 것입니다:

- 하나의 빨간 종이와 파란종이 두 장을 겹친 후 뭉쳐서 공을 만듭니다. 
- 이 종이 공이 입력 데이터이고 색종이는 분류 문제의 데이터 클래스입니다.
- 신경망이 해야할 일은 종이 공을 펼쳐서 두 클래스가 다시 깔끔하게 분리 되는 변환을 찾는 것 입니다.
- 손가락으로 종이 공을 조금씩 펼치는 것처럼 딥러닝을 사용하여 3D공간에서 간단한 변환들을 연결해 이를 구현합니다.

Figure 2.9. 복합한 데이터의 매니폴드 펼치기
![](/images/demo/Fig2-9.PNG)

- 종이공을 펼치는 것이 머신러닝이 하는 일 입니다. 
- 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 조금씩 분해하는 방식이 마치 사람이 종이공을 펼치기 위한 전략과 매우 흡사합니다.
- 심층 네트워크의 각 층은 데이터를 조금씩 풀어 주는 변환을 적용하므로, 이런 층을 깊게 쌓으면 아주 복잡한 분해 과정을 처리할 수 있습니다.
